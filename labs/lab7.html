<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lab 7: Kalman Filter for Distance Estimation </title>
    <style>
        /* General Styling */
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
            line-height: 1.6;
            color: #333;
        }
        .section-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        .section-container:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.15);
        }
        h1 {
            text-align: center;
            color: #2c3e50;
            margin-bottom: 25px;
            text-transform: uppercase;
            letter-spacing: 2px;
            font-size: 2.5em;
            animation: fadeInDown 1s ease-out;
        }
        h2 {
            color: #34495e;
            border-bottom: 3px solid #3498db;
            padding-bottom: 12px;
            margin-top: 35px;
            font-size: 1.8em;
            animation: fadeInLeft 1s ease-out;
        }
        h3 {
            color: #4a7c8c;
            margin-top: 25px;
            font-size: 1.5em;
            animation: fadeInRight 1s ease-out;
        }
        p {
            margin: 10px 0;
            color: #121111;
            font-size: 1.0em;
        }
        img {
            max-width: 80%;
            max-height: 500px;
            height: auto;
            display: block;
            margin: 10px auto;
            border: 2px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease, opacity 0.3s ease, box-shadow 0.3s ease;
        }
        img:hover {
            transform: scale(1.05);
            opacity: 0.95;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
        }
        table {
            border-collapse: collapse;
            width: 90%;
            margin: 20px auto;
            background-color: #fff;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: center;
            color: #555;
            transition: background-color 0.3s ease;
        }
        th {
            background-color: #3498db;
            color: #fff;
            font-weight: bold;
            font-size: 1.1em;
        }
        td:hover {
            background-color: #f0f0f0;
        }
        pre {
            background-color: #f8f8f8;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            border: 1px solid #ddd;
            font-size: 0.9em;
            color: #333;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }
        .equation {
                text-align: center;
                margin: 20px 0;
                font-size: 1.2em;
                color: #333333; /* Match body text */
        }
        pre:hover {
            transform: scale(1.02);
        }
        .placeholder {
            color: #888;
            font-style: italic;
            opacity: 0.7;
            transition: opacity 0.3s ease;
        }
        .placeholder:hover {
            opacity: 1;
        }
        /* Image Containers */
        .image-container, .two-image-container {
            display: flex;
            justify-content: center;
            gap: 20px;
            flex-wrap: wrap;
            margin: 10px 0;
            transition: opacity 0.5s ease, transform 0.3s ease;
        }
        .image-container:hover, .two-image-container:hover {
            transform: scale(1.02);
        }
        .image-container img, .two-image-container img {
            opacity: 0;
            animation: fadeIn 1s ease-in forwards;
            max-width: 30%; /* Fits three images side by side */
            max-height: 600px;
            height: auto;
            border: 2px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        .two-image-container img {
            max-width: 48%; /* Fits two images side by side */
            max-height: 600px;
            height: auto;
            border: 2px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        iframe {
            max-width: 80%;
            display: block;
            margin: 10px auto;
            border: 2px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        iframe:hover {
            transform: scale(1.02);
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
        }


        .code-c {
            background-color: #f9f9f9;
            color: #2c3e50;
            padding: 16px;
            border-left: 4px solid #3498db;
            border-radius: 6px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.95em;
            overflow-x: auto;
            white-space: pre-wrap;
            line-height: 1.6;
            box-shadow: 0 2px 6px rgba(0,0,0,0.08);
        }



        /* Animations */
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        @keyframes fadeInDown {
            from { opacity: 0; transform: translateY(-20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        @keyframes fadeInLeft {
            from { opacity: 0; transform: translateX(-20px); }
            to { opacity: 1; transform: translateX(0); }
        }
        @keyframes fadeInRight {
            from { opacity: 0; transform: translateX(20px); }
            to { opacity: 1; transform: translateX(0); }
        }

        /* Enhanced Responsive Design */
        @media (max-width: 1200px) {
            .section-container { max-width: 95%; padding: 15px; }
            img, iframe { max-width: 90%; max-height: 400px; }
            .image-container img, .two-image-container img { max-width: 45%; }
            table { width: 95%; }
            h1 { font-size: 2.2em; }
            h2 { font-size: 1.6em; }
            h3 { font-size: 1.3em; }
        }
        @media (max-width: 768px) {
            .section-container { max-width: 90%; padding: 10px; }
            img, iframe { max-width: 100%; max-height: 250px; }
            .image-container img, .two-image-container img { max-width: 100%; }
            table { width: 100%; }
            h1 { font-size: 1.8em; }
            h2 { font-size: 1.4em; }
            h3 { font-size: 1.1em; }
            p { font-size: 1em; }
        }
        @media (max-width: 480px) {
            .section-container { padding: 5px; }
            img, iframe { max-height: 200px; }
            pre { font-size: 0.8em; padding: 10px; }
            table { font-size: 0.9em; }
            button { padding: 8px 15px; font-size: 0.9em; }
        }
    </style>
    <script>
        // Smooth scrolling for navigation (if needed, add links later)
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth',
                    block: 'start'
                });
            });
        });

        // Image lazy loading with animation
        document.addEventListener('DOMContentLoaded', function() {
            const images = document.querySelectorAll('.image-container img, .two-image-container img');
            images.forEach(img => {
                img.style.opacity = '0';
                const observer = new IntersectionObserver((entries, observer) => {
                    entries.forEach(entry => {
                        if (entry.isIntersecting) {
                            entry.target.style.opacity = '1';
                            entry.target.style.animation = 'fadeIn 1s ease-out forwards';
                            observer.unobserve(entry.target);
                        }
                    });
                }, { threshold: 0.1, rootMargin: '50px' });
                observer.observe(img);
            });
        });

        // Dynamic table toggle for sensor data (not used here, kept for consistency)
        function toggleTable() {
            const table = document.querySelector('table');
            if (table) {
                if (table.style.display === 'none' || table.style.display === '') {
                    table.style.display = 'table';
                    table.style.animation = 'fadeIn 0.5s ease-out';
                } else {
                    table.style.animation = 'fadeOut 0.5s ease-out';
                    setTimeout(() => table.style.display = 'none', 500);
                }
            }
        }

        // Image zoom on click
        document.querySelectorAll('img').forEach(img => {
            img.addEventListener('click', function() {
                this.classList.toggle('enlarged');
                if (this.classList.contains('enlarged')) {
                    this.style.maxWidth = '90%';
                    this.style.maxHeight = '80vh';
                    this.style.position = 'fixed';
                    this.style.top = '50%';
                    this.style.left = '50%';
                    this.style.transform = 'translate(-50%, -50%)';
                    this.style.zIndex = '1000';
                    this.style.cursor = 'zoom-out';
                } else {
                    this.style.maxWidth = '80%';
                    this.style.maxHeight = '500px';
                    this.style.position = 'static';
                    this.style.top = 'auto';
                    this.style.left = 'auto';
                    this.style.transform = 'none';
                    this.style.zIndex = 'auto';
                    this.style.cursor = 'default';
                }
            });
        });

        // Fade animations for sections
        document.querySelectorAll('h1, h2, h3, p').forEach(element => {
            element.style.opacity = '0';
            const observer = new IntersectionObserver((entries, observer) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.style.opacity = '1';
                        entry.target.style.animation = 'fadeIn 1s ease-out';
                        observer.unobserve(entry.target);
                    }
                });
            }, { threshold: 0.1 });
            observer.observe(element);
        });

        // CSS animations for fadeOut
        const styles = document.createElement('style');
        styles.textContent = `
            @keyframes fadeOut {
                from { opacity: 1; }
                to { opacity: 0; }
            }
            @keyframes fadeIn {
                from { opacity: 0; transform: translateY(20px); }
                to { opacity: 1; transform: translateY(0); }
            }
            .enlarged {
                max-width: 90% !important;
                max-height: 80vh !responsivimportant;
                position: fixed !important;
                top: 50% !important;
                left: 50% !important;
                transform: translate(-50%, -50%) !important;
                z-index: 1000 !important;
                cursor: zoom-out !important;
            }`;
        document.head.appendChild(styles);
        <code>step_response()</code>
    </script>
</head>
<body>
    <div class="section-container">
        <h1>Lab 7: Kalman Filter for Distance Estimation</h1>

        <h2>Objective</h2>
        <p>
            The goal of this lab was to implement a Kalman Filter on the Artemis board to estimate the robot's velocity and position in real time using a Time-of-Flight (ToF) sensor. I aimed to command the robot to move toward a wall and stop at a target distance using a PD controller, relying solely on sensor feedback and onboard estimation.
        </p>

        <h2>Step 1: Step Response Implementation on Arduino</h2>
        <p>
            To analyze the robot’s dynamic response to a step input, I implemented a <code>step_response()</code> function that commands the motors to apply a step PWM input of 100 while logging ToF-based distance measurements and computing velocity. This allowed me to characterize how the robot accelerates and moves in response to a known control input.        </p>

        <h4>Boolean Flag Control</h4>
        <p>
            To trigger and stop the step response routine over BLE, I introduced a <code>is_step_response</code> flag to toggle logging based on BLE commands. 
            The <code>START_STEP</code> command sets it to true, and <code>STOP_STEP</code> resets it along with other logging variables.
            This approach makes it easy to run the experiment on demand without needing to reflash the board.
        </p>
        

        <h3>(i)Step Response Control Routine</h3>
        <p>
            The <code>step_response_control()</code> function executes the experiment in three phases: an idle phase (0 PWM), an active phase (100 PWM for 3s), and a final stop phase (0 PWM again). This structure captures the full acceleration-deceleration profile.
        </p>

            <img src="labs_img/Step_Response_Control _Routine.png" alt="Step Response Control Routine">

        <h3>(ii)Step Response Logging Function</h3>
        <p>
            Inside <code>step_response()</code>, I drove motors at a constant PWM in a straight line towards a pillowed wall, read ToF distance measurements and then
            logged time, PWM, distance, and velocity to arrays.
        </p>
        <img src="labs_img/Step_Response_Logging.png" alt="Step Response Control Routine">

        <P>
            This structure ensures that the robot's full response curve — from rest to steady-state — is well logged.
        </P>

        <h3>(iii)Step Response Data Collection via BLE</h3>
        <p>
            To trigger logging data from the robot, I implemeted three BLE commands. <code>START_STEP</code>, <code>STOP_STEP</code> which get's automatically 
            triggered by the is_step_response flag and a <code>COLLECT_DATA</code> to receive all data samples (Distance, Velocity and PWM for plotting)
        </p>

        <p>
            Each BLE packet contained <code>time|pwm|distance|velocity</code> which was parsed and stored.
        </p>
        <div class="two-image-container">
            <img src="labs_img/start_step_cmd.png" alt="YAW DATA">
            <img src="labs_img/collect_data_cmd.png" alt="Python BLE Notification Handler">
        </div>

        <h2>Step 2: Estimating Momentum and Drag</h2>
        <p>To tune the Kalman filter model, I needed to estimate the robot's mass-like parameter (momentum term <span class="equation">m</span>) and its damping resistance to motion (drag <span class="equation">d</span>). I assumed a first-order model of the form:</p>
        <p class="equation">v̇(t) = <span class="fraction"><span class="numerator">1</span><span class="denominator">m</span></span>u - <span class="fraction"><span class="numerator">d</span><span class="denominator">m</span></span>v(t)</p>
        <p>Solving this differential equation gives an exponential velocity curve:</p>

        <div class="equation">
            <strong>v(t) = v<sub>ss</sub> · (1 − e<sup>−t/τ</sup>)</strong>
        </div>

        <p class="note">Where:
        <ul class="note">
            <span class="equation">v_ss = <span class="fraction"><span class="numerator">u</span><span class="denominator">d</span></span></span> is the steady-state velocity and 
            <span class="equation">τ = <span class="fraction"><span class="numerator">m</span><span class="denominator">d</span></span></span> is the time constant.
        </ul>
        
        

       <h4>
        Find Steady-State Velocity and time constant 
       </h4> 

       <img src="labs_img/step_response_datas.png" alt="YAW DATA">
       <p>
        From the velocity plot, I identified a region where the velocity plateaued — between <strong>2.5s</strong> and <strong>3.0s</strong>.
        This gave us the estimated steady-state velocity:
        </p>
        
            <div class="equation">
                <strong>v<sub>ss</sub> ≈ 552.92 mm/s</strong>
            </div>
            <p>
                To estimate the time constant <strong>τ</strong>, I first computed the time <strong>t<sub>0.9</sub></strong> when the velocity first reached 90% of the steady-state velocity. This tells us how fast the robot approaches its final velocity.
            </p>
            
            <p>
                I found:
            </p>
            <div class="equation">
                <strong>t<sub>0.9</sub> ≈ 1.58 seconds</strong>
            </div>
            
            <p>
                For a first-order exponential system:
            </p>
            
            <div class="equation">
                <strong>t<sub>0.9</sub> ≈ 2.3 × τ ⇒ τ ≈ t<sub>0.9</sub> / 2.3 ≈ 0.687 s</strong>
            </div>
            
            <h4>Solving for Drag (d) and Momentum (m)</h4>
            <p>
                Using the standard model:
            </p>
            
            <div class="equation">
                <strong>v<sub>ss</sub> = u / d ⇒ d = u / v<sub>ss</sub></strong>
            </div>
            
            <div class="equation">
                <strong>τ = m / d ⇒ m = τ × d</strong>
            </div>
            
            <p>
                With values:
                <ul>
                    <li><strong>u</strong> = 100 (PWM input), 
                    <strong>v<sub>ss</sub></strong> = 552.92 mm/s and 
                    <strong>τ</strong> = 0.687 s</li>
                </ul>
            </p>
            
            <pre><code>d = 100 / 552.92   # ≈ 0.1809
m = 0.687 * d      # ≈ 0.1245 </code></pre>
            <p>
                These parameters were then used in the Kalman filter matrices for real-time estimation and control.
            </p>


        <h2>Step 3: Offline Kalman Filter Simulation</h2>
        
        <p>
            Using the calculated <b>d</b> and <b>m</b> values, 
            I modeled the robot’s dynamics and built a simulation of the Kalman filter in Python. The idea was to confirm that our estimator could track position and velocity closely even with noisy measurements — before deploying it live on the robot.
        </p>
        

        <h4>System Model </h4>
        <p>I modeled the robot as a second-order linear system where: <p>


        <img src="labs_img/model.png" alt="YAW DATA">

            <p>
                I plotted two key components (i) Position - True vs Measured (with noise) vs Estimated (Kalman filter) and (ii) Velocity - True vs Estimated
            </p>

        <img src="labs_img/kalman_filter_simulation.png" alt="YAW DATA">

        <p>
            The result showed that our Kalman filter effectively smoothed the position and velocity, matching the underlying motion even in the presence of measurement noise.
        Our simulated Kalman filter tracked both position and velocity with high fidelity. This gave me confidence to move forward and deploy the estimator on my Artemis board. I therefore used the exact same matrices A, B, H, Q, and R in my Arduino code.
        </p>



        <h2>Step 4: Real-Time Kalman Filter on Robot</h2>
        <p>
            After validating the filter offline, I then went ahead to implement it on my robot car and used it for real-time state estimation onboard the robot.
             I did this with the use of two functions , <code>kalman_update()</code> which contained the filter logic implementation which would get triggered inside <code>drive_until_target()</code> whenever new sensor data was available.
        </p>

        <img src="labs_img/drive_until_target.png" alt="YAW DATA">
        This estimator is called inside drive_until_target() every time new ToF data is ready. 
        <pre class="code-c"> <code> y = z_meas - (H[0]*x_pred[0] + H[1]*x_pred[1]);
  K[0] = ...; // Kalman Gain
  x_est[0] = x_pred[0] + K[0]*y;
  x_est[1] = x_pred[1] + K[1]*y; </code> </pre>



        <h4>PD Controller for Target-Based Driving</h4>
        <p>
            I also implemented a Proportional-Derivative (PD) controller that uses the Kalman filter estimates to compute motor PWM values. 
            The controller helps the robot slow down as it approaches a predefined target distance (30 cm) and stop smoothly. I added a new command that allowed me 
            to tune the controller remotely via BLE without having to burn arduino code everytime saving me a huge amount of time. I settled on kp of 2.5 and kd of 0.7.
        </p>


        <img src="labs_img/python_set_pd.png" alt="Python BLE Notification Handler">

        <p>
            This PD controller ensures that, te robot speeds up when far from the wall, decelerates as it nears the target, and halts automatically when the distance error or PWM falls below the set threshold.
        </p>

        <h4>
            Boolean is_kalman_drive
        </h4>
        <p>
            To control when the Kalman filter-based driving was active, I added a flag <code>is_kalman_drive</code> that gets set via BLE command. This flag ensures the filter and controller only run when explicitly requested, 
            and automatically stops the robot when reaching the target distance.
            When this flag is set to true, the robot uses its Kalman filter estimates to drive towards the target. When false, or when the target is reached, the robot stops and the filter resets.
        </p>
        <img src="labs_img/DRIVE_TO_TARGET_CMD.png" alt="Python BLE Notification Handler">
        <p>
            On the Python side, I used a BLE notification handler to collect this stream of position and velocity estimates over time.
            I logged timestamps and plotted both <strong>estimated position</strong> and <strong>estimated velocity</strong> to evaluate the Kalman filter’s live performance.    
        </p>
        <img src="labs_img/filter_arduino_results.png" alt="Kalman Filter Estimating Robot Velocity">

        <p class="note"> This result demonstrates that the Kalman filter estimates velocity in real time with high accuracy, even as the robot slows down before reaching the target distance.</p>

        <h2>Conclusion</h2>
        <p>
            Lab 7 was an exciting opportunity to bring a Kalman filter to life on a real robot. By estimating the robot’s velocity and position in real time, I was able to enhance the raw ToF sensor data and improve overall motion control. Beyond just implementation, this lab deepened my understanding of how Kalman filters work and why they’re so powerful for robotics and sensor fusion.
        </p>
        <p>
            <strong>[1]</strong> Huge thanks to Stephan Wagner (2024) for the inspiration and helpful documentation throughout this process.
        </p>
        
    </body>
    </html>
    